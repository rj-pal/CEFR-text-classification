# CEFR-text-classification

Final Project for Lighthouse Labs Data Science program. This project entails building and comparing models that takes a sample of an English language text and categorizes the level as one of five levels: A2, B1, B2, C1, and C2. These designations are five of the six levels from the Common European Framework of Reference for Languages (CEFR) 6-point industry standard scale for evaluating English proficiency.

The files in this project include:
  - Information regarding the data sets used including csv files for the second data set (csv files for the first are available locally only)
  - Python file scripts customized for this project in the directory nlp: cleaning and preprocessing
  - Jupyter Notebooks for cleaning and preparing the data for modelling
  - Jupyter Notebooks for modelling

## Data

Two data sets were used in this project:
  1. The Cambridge Readability [Data Set](https://ilexir.co.uk/datasets/index.html).<br>
  - I would like to acknowledge the creators of the data set under the Cambridge University Licence agreement.<br><br>_Menglin Xia, Ekaterina Kochmar and Ted Briscoe (2016). Text Readability Assessment for Second Language Learners. Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications._ <br>
  2. The One Stop [English Corpus](https://github.com/nishkalavallabhi/OneStopEnglishCorpus)
  - I would to acknowledge the creators of the data set under the Creative Commons Attribution-ShareAlike 4.0 International License. <br><br>_OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification Sowmya Vajjala and Ivana Lučić 2018. Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 297–304. Association for Computational Linguistic._

Both data sets were indispensable to the feasibility and success of this project and were a source of inspiration for creating tools that can help English language learners to prosper. 

## Cleaning and Preprocessing

A significant amount of time went into examining, cleaning, and preparing the data for modelling. As it was an iterative process, I created some custom functions specific to each data set in order to process and prepare them for csv formatting. In addition, I created many custom functions that could be applied to both data sets for both natural language processing and statistical analysis. The process also included manual inspection of the data sets, and I included some of the many steps in the notebook 0_data_cleaning to demonstrate a part of the process undertaken. The directory **nlp** incudes the two files: **cleaning.py**, which is catered to each data set, and **preprocessing.py**, which includes general functions for model preparation and data analysis.

## Modelling 

- *1_data_modelling notebooks*<br> 
Due to the small size of the data set, the original models on the Cambridge data set involved using both each document as a data point compared with using each sentence in each document as a data point. The later approach was done in an attempt to expand the training data. I used Bag-of-Words vectorization with Multinomial Naive Bayes models and a One-versus-Rest with Logistic Regression to establish the best baseline model. The best baseline Naive Bayes model had training accuracy around **80%** and validation accuracy around **50%** when using *each sentence as a data point in itself*. In regard to the other models, they overfit the training data, a problem which seemed to persist throughout the project.<br><br>The second set of initial models tried to overcome this lack of data by incorporating the One Stop data set, although this would come at the cost of recategorizing the initial data set into 3 categories: Elementary (A2), Intermediate (B1/B2), and Advanced (C1/C2). If any of the new models proved fruitful, the plan was to train on the One Stop Corpus and use the model to test the CEFR documents and classify ESL texts according to those three categories instead. In addition, this corpus split each reading document into roughly even pieces: one data point contained on average just under 3 sentences per segment with a total number of words around 50. Models with One Stop included Bag-of-Words and Term Frequency-Inverse Document Frequency vectorization as well as an added feature to the former of the average number of word per sentence. Tests were done with various amounts of data cleaning, for example removing or keeping stopwords or numbers. The best result here was a One-versus-Rest model using Tfidf and average number of words with the stopwords not removed. The training accuracy was around **75%** and validation accuracy was around **44%**. These results did not show improvement over the original tests on the smaller Cambridge data set. I decided to try to use neural networks to continue my testing on the One Stop data set. 

- *2_data_modelling notebooks*<br> 
To use a neural network optimally, a larger the amount of data, the better. Thus, I expanded the One Stop Corpus by breaking each document segment down into individual sentences. I believe that a text can be categorized by level based on **vocabulary** and **grammar complexity** (these are the driving forces behind the stats model in the next section). My thought was that by using a LSTM model, the neural network could figure out the sequential nature of a sentence and make a classification based on that information, with sentences from elementary readings sharing certain similar structures, and thus likewise for the other two levels. In essence, it is similar to a sentiment analysis: I tried to capture the level-sentimentality of a sentence. In the *0_data_preparation_neural_network* notebook, I split the data set into two parts for training and testing, and saved them as csv files. Then, I proceeded to building a LSTM neural network model. After finding the best dropout rate and embedding size in the neural network through a grid search, I ran a longer test. Unfortunately, the model overfit the training data and the validation data did not fair well, indicating that the neural network could only understand the data it was presented and could not use the same weights to apply to new data. See the accompanying notebooks for details. I would like to have explored this approach more with more detailed parameter tuning, but due to lack of time, I decided to return to my original approach of using stats-based models.

- *3_data_modelling notebook*<br>
For the final model, I returned to a stats-based approach working, and in the end, I only used the Cambridge data set for the final model. To capture **grammar** and **vocabulary complexity**, I two created new features. First, I found a data set at https://github.com/openlanguageprofiles/olp-en-cefrj available under a [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) that contained word lists corresponding to the CEFR levels. My thought was I could correlate the number of words at each level contained within a document to capture the vocabulary complexity of a text. I coded my own custom function to make the best use of this data set to create word counts for each level. Second, I used the nltk library to get parts-of-speech information for each sentence, and coded my own function to create parts-of-speech counts for each part-of-speech. These two features, combined with the total number of words per document and the average length of words per document yielded the best results. Models using One-versus-Rest or Gradient Boosting tended to over fit the model while maintaining relatively high validation accuracy, although I found they did not fit well to new data from some book excerpts that I had prepared.<br><br> 
    
I found the best model to be a *Multinomial Naive Bayes Model* where the **bias-variance trade off** was optimized at around **70%** (*0.7205* and *0.6969* respectively). Most misclassifications were between adjacent levels. The model was pretty accurate in distinguishing between A2 and B1 levels, and alright between C1 and C2; however, most misclassification occurred amongst the highest three levels (B2, C1, C2) with texts at the B2 level being the hardest to classify accurately. I tried these same steps on the One Stop data set, yet did not yield any more accurate results. Although the final result does not have the greatest accuracy, given the limited size of the data set, and compared to other studies using stats-based [features](https://www.cl.cam.ac.uk/~ek358/BEA_Final.pdf), I thought they were decent.

## Future Prospects

I hope to continue to refine my stats-based model by engineering new features based on ratios of the word counts, as well as other lexical or syntactic features. I would like to incorporate and test those features in addition to trying ensemble models best capture the vocabulary and grammar complexity of ESL texts in order to make more accurate classifications. I would also like to explore a neural network approach in more detail, and even explore the possibility of using different word embeddings, like Word2Vec or Transformers. The cutting edge research and results in natural language processing use transformers, and ideally, I would like to explore some of the models that [hugging face](https://huggingface.co/models) have to offer. Finally, I would like to build a website or app where English language learners can take any snippet of text: a book excerpt, a blog post, a paragraph from an online magazine, or even a tweet - put it into my engine and return the approximate level of the text. This way, self-directed learner will be able to find the best reading learning material to enhance and grow their language proficiency. 
