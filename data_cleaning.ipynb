{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce182e0-e555-4aaf-a4ae-9894f95568ac",
   "metadata": {},
   "source": [
    "### Data Cleaning and Processing: Cambridge English Readability Data Set and the One Stop English Corpus\n",
    "\n",
    "This notebook showcases the cleaning process I undertook to prepare the *Cambridge English Readability Data Set* for data analysis. \n",
    "\n",
    "The data set can be found here: https://ilexir.co.uk/datasets/index.html\\\n",
    "\n",
    "I would like to acknowledge the authors below per se the licence agreement and that the data set is used solely for learning purposes.\n",
    "\n",
    "Citation:\n",
    "\n",
    "*Yannakoudakis, Helen and Briscoe, Ted and Medlock, Ben, ‘A New Dataset and Method for Automatically Grading ESOL Texts’<br> Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5de7e-81d8-41bd-b005-b9ffe421e516",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "Cleaning of the *Cambridge English Readability Data Set* was an iterative process of manual text examination through python looping. Because the size was manageable, my data cleaning included, for some examples, taking notes of certain words or phrases causing probelms, like list header items, as well as the examining the lengths of title lines to make a cut-off for texts that didn't have titles while not deleting texts that had no title with shorter lengths. I included some of the code testing blocks commented out, to get a sense of the cleaning process. I believe I did a pretty good job of cleaning the documents, though I also note that some list items and other small words that should be deleted were missed and an even more thourough cleaning should be conducted.\n",
    "\n",
    "Deleted files **FCE/19.txt, FCE/28.txt, FCE/32.txt** as they were duplicates of **FCE/20.txt, FCE/29.txt, FCE/33.txt**. \n",
    "\n",
    "Modified **PET/34.txt** due to lack of spacing in two sentences.\n",
    "\n",
    "All cleaning functions can be found in the *cleaning_nlp.py* file\n",
    "\n",
    "Otherwise, the original files have been untouched and only cleaned after loading and processing through the process_directory function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f56d1b-143b-4d47-bf23-9a8908d8a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cleaning_nlp as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7557a3e-2526-45b9-a08f-96b7187b0ed8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: KET\n",
      "\n",
      "Removed First line: Otters \n",
      "\n",
      "Removed First line: BICYCLES \n",
      "\n",
      "Removed First line: Bill Prince-Smith\n",
      "\n",
      "Removed First line: ESTHER'S STORY\n",
      "\n",
      "Removed First line: A HISTORY OF AIR TRAVEL \n",
      "\n",
      "Removed First line: CANADA GEESE\n",
      "\n",
      "Removed First line: Memo \n",
      "\n",
      "Removed: Memo\n",
      "\n",
      "Removed First line: BURGLARS LOVE THE AFTERNOON\n",
      "\n",
      "Removed First line: CROCODILES \n",
      "\n",
      "Removed First line: Madame Tussaud's\n",
      "\n",
      "Removed First line: The Weather \n",
      "\n",
      "Removed First line: The Elephant Show \n",
      "\n",
      "Removed: by Daniel Allsop, age 14 \n",
      "\n",
      "... ... ...",
      "\n",
      "Removed First line: The Heat is On\n",
      "\n",
      "Removed First line: Music - The Challenge Ahead\n",
      "\n",
      "Removed First line: Metals\n",
      "\n",
      "Removed First line: Work\n",
      "\n",
      "Removed First line: The Lure of the Kitchen\n",
      "\n",
      "Removed: SAILING\n",
      "\n",
      "Removed First line: BROADCASTING: The Social Shaping of a Technology\n",
      "\n",
      "Removed First line: 0ral History\n",
      "\n",
      "CPE has 69 files\n",
      "\n",
      "Number of First Line Deletions: 59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documents, levels, first_line_lens = cl.process_directory()\n",
    "# documents, doc_list, levels, first_line_lens = cl.process_directory(cefr=False) # -> Created second version just as back-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d93b0c-30fd-41fb-a52c-576a5ec3d2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final Manual Inspection after Cleaning\n",
    "# for row in cl.cefr_to_data_frame(documents, levels)['documents']:\n",
    "#     print(row)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381999b0-cb96-4150-b773-791e223acde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = cl.cefr_to_data_frame(documents, levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618c507a-18e8-4f6f-b9b1-17109cb7209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d8dae6-79d6-44d0-afca-d73eb3d59f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned data to csv file\n",
    "\n",
    "# dataframe.to_csv('data/cefr_readings.csv', index=False)\n",
    "# dataframe.to_csv('data/cefr_readings_numeric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792c182-a2be-4cec-b85a-f3179dffd967",
   "metadata": {},
   "source": [
    "#### Note \n",
    "\n",
    "The data set below was processed using my own functions and put into lists in tuples with various stored statistics on the data set. I originally parsed the directory */Texts-Together-OneCSVperFile* (found in the github link below) and loaded the csv files into one data frame, but I noticed that there were lots of spacing issues in many of the sentences. I changed course and processed the text files instead. \n",
    "\n",
    "Because of the enormity of the data set, I could not manually look through it all. However, after looking through various random ranges of samples, I concluded that the spacing isssue was not present and that the data looked pretty clean.\n",
    "\n",
    "The One Stop English Corpus can be found here: https://github.com/nishkalavallabhi/OneStopEnglishCorpus\n",
    "\n",
    "I would like to acknowledge the authors below per se the licence agreement and that the data set is used solely for learning purposes.\n",
    "\n",
    "Citation:\n",
    "\n",
    "*OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification Sowmya Vajjala and Ivana Lučić 2018<br>\n",
    "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 297–304. Association for Computational Linguistics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caddcc07-4466-4949-baf2-d466e492a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_stop_df = cl.get_one_stop_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b90a12c-d284-40f8-8cb8-00684ed05e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ab823d-e4d3-4e81-a06e-2057e8c4cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_stop_df.to_csv('data/one_stop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3048a854-0796-49ae-9fcc-2189f8bbb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(6800, 7300, 2):\n",
    "#     print(one_stop_df.documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91389d4-6519-4403-8038-6e4555e6ed47",
   "metadata": {},
   "source": [
    "### Data Cleaning Inspection \n",
    "Below are some samples of earlier work from the data inspection which helped inform me how to clean and make my process_directory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0eb92e-35b0-4264-8635-c3770ad6e807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspection of first lines used to determine the cut off point for first line lengths\n",
    "# for i, e in enumerate(first_line_lens):\n",
    "#     if (e > 45):# and (e > 54):\n",
    "#         print(i, e)\n",
    "#         print(documents[i][:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e57a287-735b-49f0-a419-ed2d5d1b265f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspection code before final inspection\n",
    "# c = 0\n",
    "# for i in range(len(documents)):\n",
    "#     print('********Document {n} ***********'.format(n=i))\n",
    "#     print(documents[i])\n",
    "#     if c == 190:\n",
    "#         break\n",
    "#     c += 1\n",
    "\n",
    "# for i in range(200, 300, 1):\n",
    "#     print('********Document {n} ***********'.format(n=i))\n",
    "#     print(docs[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
